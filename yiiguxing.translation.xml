<application>
  <component name="AppStorage">
    <option name="newTranslationDialogWidth" value="594" />
    <option name="newTranslationDialogX" value="976" />
    <option name="newTranslationDialogY" value="243" />
    <histories>
      <item value="Note that this is equivalent to what used to be called multinomial" />
      <item value="Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian." />
      <item value="Computes log probability of actions given distribution." />
      <item value="for getting randomized tau in training" />
      <item value="logits are equivalent after subtracting a common number" />
      <item value="an optional list of integers specifying outer dimensions to add to the spec shape before sampling." />
      <item value="This should be a deterministic function converting b1_a to b1_tau" />
      <item value="Create as a buffer so that training from a checkpoint will have the correct flag." />
      <item value="td return till the first action switching" />
      <item value="masks indicating which samples are valid." />
      <item value="The current second derivative of action" />
      <item value="For policy evaluation, TAAC uses a compare-through Q operator for TD backup by re-using state-action sequences that have shared actions between rollout and training." />
      <item value="for example, it can be a constant function representing the same action, or a quadratic function." />
      <item value="Different sub-algorithms implement different forms of the 'trajectory' concept," />
      <item value="nutsell" />
      <item value="TaacL's trajectory is piece-wise linear. Each time the policy decides whether to repeat the previous linear traj or generate a new one. Importantly, to generate a new one the policy doesn't directly generate the entire set of two parameters :math:`(a,v)` because this will result in bad exploration in the action space. Instead," />
      <item value="grocery" />
      <item value="a tuple of integers representing hidden FC layer sizes FC layers after merging observations and actions." />
      <item value="A bool to gather debug summaries." />
      <item value="a big number to make sure the TimeLimit wrapper ends the game" />
      <item value="port" />
      <item value="10Hz control freq" />
      <item value="egocentric" />
      <item value="agent's egocentric distance and direction to goal egocentric_perception_range (float): the max range in degree to limit the agent's observation." />
      <item value="For the non-image observation case, use the states transformed to egocentric coordinate" />
      <item value="use egocentric states" />
      <item value="The returned environment should not access global variables." />
      <item value="the code assumes that all spaces under the nested observation space is a Box space." />
      <item value="Flattens selected keys of a Dict observation space into an array." />
      <item value="Loads the selected environment and wraps it with the specified wrappers." />
      <item value="Retrieve the success info from the environment return" />
      <item value="training batches per cycle" />
      <item value="Create a context object for recording time." />
      <item value="Calculate the reward actually used for training." />
      <item value="replace reward in `experience` with the freshly computed intrinsic rewards by the goal generator during `train_step`." />
      <item value="eplace reward in `experience` with the freshly computed intrinsic rewards by the goal generator during `train_step`." />
      <item value="dubbed" />
      <item value="The initialized network parameters will be different." />
      <item value="Parallel Critic Network" />
      <item value="These paras will train themselves, so let the parent algorithm ignore them" />
      <item value="extra" />
      <item value="rets" />
      <item value="Expand the shape of ``x`` with extra singular dimensions." />
      <item value="Trusted Updater" />
      <item value="Make sure to instantiate with parenthesis.)" />
      <item value="It will reduce memory consumption for computations that would otherwise have `requires_grad=True`." />
      <item value="Used for annotating the decorator usage of 'no_grad' and 'enable_grad'." />
      <item value="Allow a context manager to be used as a decorator" />
      <item value="This buffer doesn't preserve temporality as data from multiple environments will be arbitrarily stored." />
      <item value="Stop waiting processes from being blocked." />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="16" />
        <entry key="ENGLISH" value="17" />
      </map>
    </option>
  </component>
  <component name="Cache">
    <option name="lastTrimTime" value="1638174421029" />
  </component>
  <component name="Settings">
    <option name="foldOriginal" value="true" />
  </component>
</application>