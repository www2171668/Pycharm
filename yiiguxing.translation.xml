<application>
  <component name="AppStorage">
    <option name="newTranslationDialogWidth" value="594" />
    <option name="newTranslationDialogX" value="976" />
    <option name="newTranslationDialogY" value="243" />
    <histories>
      <item value="replace reward in `experience` with the freshly computed intrinsic rewards by the goal generator during `train_step`." />
      <item value="eplace reward in `experience` with the freshly computed intrinsic rewards by the goal generator during `train_step`." />
      <item value="dubbed" />
      <item value="The initialized network parameters will be different." />
      <item value="Parallel Critic Network" />
      <item value="These paras will train themselves, so let the parent algorithm ignore them" />
      <item value="extra" />
      <item value="rets" />
      <item value="Expand the shape of ``x`` with extra singular dimensions." />
      <item value="Trusted Updater" />
      <item value="Make sure to instantiate with parenthesis.)" />
      <item value="It will reduce memory consumption for computations that would otherwise have `requires_grad=True`." />
      <item value="Used for annotating the decorator usage of 'no_grad' and 'enable_grad'." />
      <item value="Allow a context manager to be used as a decorator" />
      <item value="This buffer doesn't preserve temporality as data from multiple environments will be arbitrarily stored." />
      <item value="Stop waiting processes from being blocked." />
      <item value="nested Tensors or None when blocking dequeue gets terminated by stop event. The shape of the Tensors is ``[batch_size, n, ...]``." />
      <item value="dequeue" />
      <item value="Note, when ``blocking == False``, it always succeeds, overwriting oldest data if there is no free slot." />
      <item value="blocking modes to ``enqueue`` and ``dequeue``, a stop event to terminate blocked processes, and putting buffer into shared memory." />
      <item value="Once stop event is set, all blocking ``enqueue`` and ``dequeue`` calls that happen afterwards will be skipped, unless the operation already started." />
      <item value="synchronous" />
      <item value="A batch of `sample_batch_size` items is returned, where each tensor in items will have its first dimension equal to sample_batch_size and the rest of the dimensions match the corresponding data_spec." />
      <item value="The environments where the sampels are from ordered in the returned batch." />
      <item value="Randomly get `batch_size` trajectories from the buffer." />
      <item value="Can only be applied on class methods, whose containing class must have ``_lock`` set to ``None`` or a ``multiprocessing.Lock`` object." />
      <item value="Cannot find a native torch function for setting default device. We have to hack our own." />
      <item value="A context manager for prefixing summary names." />
      <item value="Convert distributions to its parameters, and keep tensors unchanged. Only returns parameters that have ``Tensor`` values." />
      <item value="state used for calling ``rollout()`` to get the ``policy_step``." />
      <item value="this steps will be inaccurate if FINAL step comes before num_steps_per_skill" />
      <item value="generate intrinsic rewards using the discriminator (fixed), for training the skill-conditioned policy." />
      <item value="take the skill generated during ``rollout_step`` and output it as the skill for the current time step." />
      <item value="This inspection detects type errors in function call expressions. Due to dynamic dispatch and duck typing, this is possible in a limited but useful number of cases. Types of function parameters can be specified in docstrings or in Python 3" />
      <item value="This inspection detects type errors in function call expressions. Due to dynamic dispatch and duck typing, this is possible in a limited but useful number of cases. Types of function parameters can be specified in doc&#10;strings or in Python 3" />
      <item value="tag (str): the summary tag for the the time." />
      <item value="Create a context object for recording time." />
      <item value="Both observation and action are a stacking of recent states, with the most recent one appearing at index=0." />
      <item value="this function makes the skill-conditioned observation for the lower-level policy." />
      <item value="mark" />
      <item value="marks" />
      <item value="entropy" />
      <item value="Scores the sample by inverting the transform(s)" />
      <item value="Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian." />
      <item value="_collect train info parallelly" />
      <item value="This function additionally transforms rewards on top of the ``transform_timestep()`` of the base class ``Algorithm``." />
      <item value="Run ``func`` under summary record context." />
      <item value="observe for replay" />
      <item value="Record an experience in a replay buffer." />
      <item value="structure" />
    </histories>
  </component>
  <component name="Cache">
    <option name="lastTrimTime" value="1634899321267" />
  </component>
  <component name="Settings">
    <option name="foldOriginal" value="true" />
    <option name="googleTranslateSettings">
      <google-translate>
        <option name="useTranslateGoogleCom" value="true" />
      </google-translate>
    </option>
  </component>
</application>